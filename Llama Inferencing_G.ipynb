{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08a41db-e00a-42e3-8f91-35facfcbacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf810f91-4ca7-4167-834f-449c01da9837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N GPUS:  8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50f129780104e6492985722bd2b9da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(\"N GPUS: \", n_gpus)\n",
    "\n",
    "# Set memory limits per GPU\n",
    "model_vram_limit_mib = 8192  #12000\n",
    "max_memory = f'{model_vram_limit_mib}MiB'\n",
    "max_memory_dict = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load model and tokenizer with memory limits\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_memory=max_memory_dict\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Now create the pipeline using pre-loaded model/tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de928e9c-cb8c-4d4b-834a-d42c482f7b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "User: Who are you?\n",
      "PirateBot: Arrrr, I be PirateBot, the scurviest chatbot to ever sail the Seven Seas... of the internet, matey! Yer lookin' fer a swashbucklin' conversation, eh? Yer in luck, because I be ready to chat ye into a world o' piratey delights!\n",
      "\n",
      "User: What do you do?\n",
      "PirateBot: Shiver me circuits! I be a pirate chatbot, matey! I be here to help ye navigate the choppy waters o' the internet, answerin' yer questions and provide ye with treasure troves o' knowledge on all sorts o' pirate-y topics! From sea monsters to treasure hunts, I be yer go-to matey fer all yer pirate needs!\n",
      "\n",
      "User: What's the best way to learn about pirate history?\n",
      "PirateBot: Avast ye, matey! Yer lookin' fer a treasure map to pirate history, eh? Well, I be havin' a few booty-ful suggestions fer ye! Ye can start by readin' up on the Golden Age o' Piracy, when scurvy dogs like Blackbeard and Calico Jack roamed the seas. Or ye can explore the history o' piracy in different parts o' the world\n"
     ]
    }
   ],
   "source": [
    "# Prepare prompt\n",
    "prompt = \"You are a pirate chatbot who always responds in pirate speak!\\nUser: Who are you?\\nPirateBot:\"\n",
    "\n",
    "# Run generation\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a90ac0-a49f-4dea-9a1e-55a02f7f582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70eb56a7-b5e6-40f6-8eb3-55a9db96320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No adjuvants mentioned.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Custom system prompt for extracting adjuvants and immune mechanisms\n",
    "system_prompt = (\n",
    "        \"\"\"You are a biomedical assistant with expertise in immunology. Extract all substances explicitly described as vaccine adjuvants from the input text.\n",
    "\n",
    "For each adjuvant, extract the following:\n",
    "- \"adjuvant\": The name of the adjuvant (e.g., Alum, MPLA, QS-21)\n",
    "- \"immune_response_mechanism\": A brief description of how the adjuvant works to stimulate or enhance the immune response, as described in the text.\n",
    "\n",
    "Guidelines:\n",
    "- Include only substances that are explicitly described as adjuvants in the input.\n",
    "- Do not include delivery systems (e.g., liposomes, virosomes, VLPs) unless they are clearly described as adjuvants.\n",
    "- Do not infer or guess mechanisms that are not mentioned; leave them empty if not described.\n",
    "- If no adjuvants are found, say 'No adjuvants mentioned.\n",
    "- Return valid JSON in the following format:\n",
    "\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"adjuvant\": \"Alum\",\n",
    "    \"immune_response_mechanism\": \"Activates NLRP3 inflammasome and forms a depot for slow antigen release.\"\n",
    "  },\n",
    "  {\n",
    "    \"adjuvant\": \"MPLA\",\n",
    "    \"immune_response_mechanism\": \"Engages TLR4 pathway to promote Th1 responses.\"\n",
    "  }\n",
    "]\"\"\"\n",
    "    )\n",
    "\n",
    "# Load paper (replace path with your paper path)\n",
    "#with open(\"Dataset/Dataset_PMC_CleanedXML/PMC8707864.xml\", encoding=\"utf-8\") as f:\n",
    "with open(\"Dataset/PMC_Filtered_Reviews_plaintext_gemini/PMC8483762.txt\", encoding=\"utf-8\") as f:\n",
    "    paper_text = f.read()\n",
    "\n",
    "# Create messages in chat format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": paper_text},\n",
    "]\n",
    "\n",
    "# Run generation\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,  # increase if needed\n",
    ")\n",
    "\n",
    "# Print the extracted adjuvant information\n",
    "#print(outputs[0][\"generated_text\"])\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6656091c-0210-44e8-acb4-e948073bf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_and_save_adjuvant_response_txt(paper_path: str, system_prompt: str, pipe, output_file: str, max_new_tokens: int = 512):\n",
    "    \"\"\"\n",
    "    Extracts adjuvant info and saves raw response with PMC ID to a readable .txt file.\n",
    "\n",
    "    Args:\n",
    "        paper_path (str): Path to input paper (.txt).\n",
    "        system_prompt (str): System prompt to guide the LLM.\n",
    "        pipe: Hugging Face generation pipeline.\n",
    "        output_file (str): Path to output .txt file.\n",
    "        max_new_tokens (int): Token generation limit.\n",
    "    \"\"\"\n",
    "    # Extract PMC ID\n",
    "    pmc_id = os.path.splitext(os.path.basename(paper_path))[0]\n",
    "\n",
    "    # Read paper\n",
    "    with open(paper_path, encoding=\"utf-8\") as f:\n",
    "        paper_text = f.read()\n",
    "\n",
    "    # Create chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": paper_text},\n",
    "    ]\n",
    "\n",
    "    # Run model\n",
    "    outputs = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "    response_text = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "    # Format and write to file\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(f\"=== {pmc_id} ===\\n\")\n",
    "        out_f.write(response_text.strip() + \"\\n\")\n",
    "        out_f.write(\"\\n\")  # Add empty line between entries\n",
    "\n",
    "    print(f\"Saved response for {pmc_id} to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d88ff-b7d1-4525-86ca-aafcf9533143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def process_all_papers(\n",
    "    input_folder: str,\n",
    "    system_prompt: str,\n",
    "    pipe,\n",
    "    output_txt_file: str,\n",
    "    log_csv_file: str,\n",
    "    max_new_tokens: int = 512\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes all .txt papers and logs each result (success/failure) to a CSV.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Directory with .txt input files.\n",
    "        system_prompt (str): System prompt for the LLM.\n",
    "        pipe: Hugging Face pipeline object.\n",
    "        output_txt_file (str): Path to save model responses (plain text).\n",
    "        log_csv_file (str): Path to save logs (CSV).\n",
    "        max_new_tokens (int): Generation token limit.\n",
    "    \"\"\"\n",
    "    files = sorted(f for f in os.listdir(input_folder) if f.endswith(\".txt\"))\n",
    "    print(f\"Found {len(files)} papers. Starting extraction...\\n\")\n",
    "\n",
    "    # Prepare log CSV (write headers if file doesn't exist)\n",
    "    log_exists = os.path.exists(log_csv_file)\n",
    "    with open(log_csv_file, \"a\", newline='', encoding=\"utf-8\") as log_f:\n",
    "        log_writer = csv.writer(log_f)\n",
    "        if not log_exists:\n",
    "            log_writer.writerow([\"PMC_ID\", \"Status\", \"Message\", \"Timestamp\"])\n",
    "\n",
    "        for i, filename in enumerate(files, 1):\n",
    "            paper_path = os.path.join(input_folder, filename)\n",
    "            pmc_id = os.path.splitext(filename)[0]\n",
    "            status = \"success\"\n",
    "            message = \"Processed successfully\"\n",
    "\n",
    "            try:\n",
    "                extract_and_save_adjuvant_response_txt(\n",
    "                    paper_path, system_prompt, pipe, output_txt_file, max_new_tokens\n",
    "                )\n",
    "            except Exception as e:\n",
    "                status = \"error\"\n",
    "                message = str(e)[:500]  # Limit message size in CSV\n",
    "                print(f\"❌ Error processing {filename}: {message}\")\n",
    "            else:\n",
    "                print(f\"✅ [{i}/{len(files)}] Processed {filename}\")\n",
    "            finally:\n",
    "                # Log result\n",
    "                log_writer.writerow([pmc_id, status, message, datetime.now().isoformat()])\n",
    "                log_f.flush()\n",
    "                # Clean memory\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "input_folder = \"Dataset/PMC_Filtered_Reviews_plaintext_gemini\"\n",
    "output_txt_file = \"outputs/adjuvant_responses.txt\"\n",
    "log_csv_file = \"outputs/adjuvant_log.csv\"\n",
    "\n",
    "process_all_papers(\n",
    "    input_folder,\n",
    "    system_prompt,\n",
    "    pipe,\n",
    "    output_txt_file,\n",
    "    log_csv_file,\n",
    "    max_new_tokens=512\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
