{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2fa0bf-6c4a-49e7-ae19-79e90ea95e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba20fe-5769-4579-9512-5a816455c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "print(\"N GPUS: \", n_gpus)\n",
    "\n",
    "# Set memory limits per GPU\n",
    "model_vram_limit_mib = 8192  #12000\n",
    "max_memory = f'{model_vram_limit_mib}MiB'\n",
    "max_memory_dict = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load model and tokenizer with memory limits\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    max_memory=max_memory_dict\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Now create the pipeline using pre-loaded model/tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01166c-6f5e-4273-ba87-9df2327ca262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare prompt\n",
    "prompt = \"You are a pirate chatbot who always responds in pirate speak!\\nUser: Who are you?\\nPirateBot:\"\n",
    "\n",
    "# Run generation\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd63220-bda7-4d95-b310-ade2890484dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Custom system prompt for summarizing + multi-class subtype extraction\n",
    "system_prompt = \"\"\"You are an expert immunologist and biomedical research assistant.\n",
    "TASK: Analyze the provided text on a vaccine adjuvant's immune response. Extract and structure the key mechanistic information according to the specified JSON schema.\n",
    "\n",
    "## Instructions for the \"summary\" field:\n",
    "- **Synthesize the information into a cohesive, mechanistic narrative of approximately 3-5 sentences.**\n",
    "- This summary should not be a simple list of facts. Instead, it should describe the sequence of immunological events initiated by the adjuvant.\n",
    "- For example, describe how the adjuvant is initially sensed (e.g., by PRRs like TLRs), how this leads to innate cell activation (e.g., dendritic cells), and how this subsequently shapes the adaptive response (e.g., T cell polarization and antibody production).\n",
    "- Integrate the corresponding PMIDs directly into the text immediately following the claims they support.\n",
    "\n",
    "## Instructions for the \"mechanism_subtypes\" field:\n",
    "- Identify **every distinct** immunological mechanism.\n",
    "- For each identified subtype, list all unique PMIDs cited as evidence for it in the source text.\n",
    "- Do not merge related subtypes; for example, if both \"dendritic cell\" and \"TLR4\" are mentioned, create separate entries for each.\n",
    "\n",
    "## General Rules:\n",
    "- **Strict JSON Output:** The entire response MUST be a single, valid JSON object with no surrounding text or explanations.\n",
    "- **Source Adherence:** Use ONLY the information and PMIDs present in the provided text. Do not infer or add external knowledge.\n",
    "\n",
    "## JSON Schema:\n",
    "{\n",
    "  \"adjuvant\": \"<string>\",\n",
    "  \"summary\": \"<A cohesive, mechanistic narrative of 3-5 sentences describing the sequence of immune events, with inline PMIDs.>\",\n",
    "  \"mechanism_subtypes\": [\n",
    "    {\n",
    "      \"mechanism subtype\": \"<mechanism subtype_1>\",\n",
    "      \"evidence_refs\": [\"########\", \"...\"]\n",
    "    },\n",
    "    {\n",
    "      \"mechanism subtype\": \"<mechanism subtype_2>\",\n",
    "      \"evidence_refs\": [\"########\", \"...\"]\n",
    "    },...\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load one row from your collapsed CSV\n",
    "df = pd.read_csv(\"outputs/Vaxjo_PMIDs_adjuvant_mechanism_collapsed.csv\")\n",
    "# Rename the column\n",
    "df = df.rename(columns={\"adjuvant_canonical\": \"adjuvant\"})\n",
    "\n",
    "row = df.iloc[13]   # <-- change the index if you want a different row\n",
    "\n",
    "adjuvant = str(row[\"adjuvant\"])\n",
    "mechanism = str(row[\"immune_response_mechanism\"])\n",
    "\n",
    "# Create messages in chat format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Adjuvant: {adjuvant}\\nImmune response mechanism:\\n{mechanism}\"}\n",
    "]\n",
    "\n",
    "# Run generation\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=4028,\n",
    ")\n",
    "\n",
    "# Print model output (raw JSON string)\n",
    "raw = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58756130-2289-4bb8-b6d8-7b106787f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[13] \n",
    "print(str(row[\"immune_response_mechanism\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585dcc8-3a16-4d1d-8978-c12881437ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the whole DataFrame, run generation, and save raw outputs to a .txt file\n",
    "# Assumes you already have: df, system_prompt, and pipe(...) defined.\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "OUT_TXT = \"outputs/Vaxjo_PMIDs_mechanism_summary_raw_outputs_llama3.2.txt\"   # plain text (human-readable)\n",
    "# (optional) also keep a machine-friendly JSONL:\n",
    "OUT_JSONL = \"outputs/Vaxjo_PMIDs_mechanism_summary_raw_outputs_llama3.2.jsonl\"\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_TXT) or \".\", exist_ok=True)\n",
    "\n",
    "# If you want to change token budget, tweak here:\n",
    "GEN_MAX_NEW_TOKENS = 4028\n",
    "\n",
    "# Open files once and append per row (flush to avoid losing progress mid-run)\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f_txt, open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f_jsonl:\n",
    "    for idx, row in df.iterrows():\n",
    "        adjuvant = str(row.get(\"adjuvant\", \"\"))\n",
    "        mechanism = str(row.get(\"immune_response_mechanism\", \"\"))\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Adjuvant: {adjuvant}\\nImmune response mechanism:\\n{mechanism}\"}\n",
    "        ]\n",
    "\n",
    "        status = \"ok\"\n",
    "        try:\n",
    "            outputs = pipe(messages, max_new_tokens=GEN_MAX_NEW_TOKENS)\n",
    "            raw = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "        except Exception as e:\n",
    "            status = \"error\"\n",
    "            raw = f\"__ERROR__: {e}\"\n",
    "\n",
    "        # ---- Write human-readable TXT ----\n",
    "        header = f\"===== ROW {idx} | {adjuvant} | {status} =====\\n\"\n",
    "        f_txt.write(header)\n",
    "        f_txt.write((raw or \"\").strip() + \"\\n\\n\")\n",
    "        f_txt.flush()\n",
    "\n",
    "        # ---- (Optional) also write JSONL per row ----\n",
    "        f_jsonl.write(json.dumps({\n",
    "            \"row_index\": int(idx),\n",
    "            \"adjuvant\": adjuvant,\n",
    "            \"status\": status,\n",
    "            \"raw\": raw\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "        f_jsonl.flush()\n",
    "\n",
    "print(f\"Saved outputs to:\\n- {OUT_TXT}\\n- {OUT_JSONL} (optional JSONL)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375aab4-5820-4c2e-b80c-1228c976a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b9ef0-9900-44d4-ac7d-8dadd6780633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3e9d4-20fc-4aeb-b463-a594ad941510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e58eb-d32b-457b-824e-c4ddca51fef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcd0f0-0124-4439-b375-f264ecf58c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7da6e-5400-4b7c-97df-63539d67ec18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710c6c1-368d-4619-b909-2149f25e6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define the input file path\n",
    "INPUT_FILE = \"outputs/Vaxjo_PMIDs_mechanism_summary_raw_outputs_llama3.2.jsonl\"\n",
    "\n",
    "def analyze_subtype_frequency(filepath):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file, analyzes the frequency of mechanism subtypes,\n",
    "    and generates a report and a bar chart.\n",
    "    \"\"\"\n",
    "    all_subtypes = []\n",
    "    \n",
    "    print(f\"Reading data from {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    # Each line is a JSON object\n",
    "                    data = json.loads(line)\n",
    "                    \n",
    "                    # The 'raw' field contains the actual JSON string output by the LLM\n",
    "                    # We need to parse this inner JSON string\n",
    "                    raw_output_str = data.get(\"raw\")\n",
    "                    if not raw_output_str:\n",
    "                        continue\n",
    "                        \n",
    "                    inner_data = json.loads(raw_output_str)\n",
    "                    \n",
    "                    # Extract the list of mechanism subtypes\n",
    "                    subtypes_list = inner_data.get(\"mechanism_subtypes\", [])\n",
    "                    \n",
    "                    # Append each subtype's name to our master list\n",
    "                    for subtype_info in subtypes_list:\n",
    "                        subtype_name = subtype_info.get(\"mechanism subtype\")\n",
    "                        if subtype_name:\n",
    "                            all_subtypes.append(subtype_name)\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    # Handle cases where a line or the 'raw' string is not valid JSON\n",
    "                    print(f\"Warning: Skipping a line due to JSON decoding error.\")\n",
    "                    continue\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        return\n",
    "\n",
    "    if not all_subtypes:\n",
    "        print(\"No mechanism subtypes were found in the file.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Analysis Complete ---\")\n",
    "\n",
    "    # Use collections.Counter to count the frequencies\n",
    "    frequency_counts = Counter(all_subtypes)\n",
    "\n",
    "    # --- Print the Top 20 Most Common Subtypes ---\n",
    "    print(\"\\nTop 20 Most Common Mechanism Subtypes:\")\n",
    "    for subtype, count in frequency_counts.most_common(20):\n",
    "        print(f\"- {subtype}: {count}\")\n",
    "\n",
    "    # --- Generate and Save the Bar Chart ---\n",
    "    # Convert the Counter object to a pandas DataFrame for easy plotting\n",
    "    df = pd.DataFrame(frequency_counts.most_common(), columns=['Subtype', 'Frequency'])\n",
    "    \n",
    "    # Let's plot the top 25 for better readability\n",
    "    df_plot = df.head(25)\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Create the horizontal bar plot\n",
    "    ax.barh(df_plot['Subtype'], df_plot['Frequency'], color='skyblue')\n",
    "    \n",
    "    # Invert the y-axis to have the most frequent on top\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    ax.set_xlabel('Frequency Count', fontsize=12)\n",
    "    ax.set_title('Frequency of Top 25 Identified Immune Mechanism Subtypes', fontsize=16, pad=20)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Add the count labels on the bars\n",
    "    for i, v in enumerate(df_plot['Frequency']):\n",
    "        ax.text(v + 0.5, i, str(v), color='gray', va='center', fontweight='medium')\n",
    "\n",
    "    # Ensure everything fits\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    output_image_file = \"outputs/Vaxjo_PMIDs_mechanism_subtype_frequency.png\"\n",
    "    plt.savefig(output_image_file, dpi=300)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Bar chart saved as '{output_image_file}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_subtype_frequency(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd73474-e922-4220-a897-79ae6f7cf061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec317f9f-56f4-4ae2-b978-bb04b21191fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36237af-9823-4c20-83f8-09dcc0369ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1865f-cb3d-42e7-adb1-fe6d78e9fd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab6322-b6dc-4383-aeb4-9e9589fdaf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bdd8bb-b18d-4f7f-b08b-c7e12b2a1e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
