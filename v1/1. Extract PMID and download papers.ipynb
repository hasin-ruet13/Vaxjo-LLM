{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038c90c-b408-4da5-b711-120266be425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "file_path = \"Dataset/Vaxjo/VO ID term editing.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Print the column names\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a112a-9412-466c-a633-9a0b6df2d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Bio import Entrez\n",
    "\n",
    "# Always set your email for NCBI API usage\n",
    "Entrez.email = \"your_email_here\"\n",
    "\n",
    "# Pick the column with sources\n",
    "col = \"definition source\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004256b-d003-40f8-8998-cf0ea8517fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "FILE_PATH = \"Dataset/Vaxjo/VO ID term editing.xlsx\"\n",
    "COL = \"definition source\"\n",
    "HTTP_TIMEOUT = 6\n",
    "\n",
    "# ---------- Regexes ----------\n",
    "PMID_EXPLICIT = [\n",
    "    r\"(?i)PMID[:\\s]*([0-9]+)\",\n",
    "    r\"(?i)Pubmed[:\\s]*([0-9]+)\",\n",
    "    r\"(?i)pubmed\\.ncbi\\.nlm\\.nih\\.gov/([0-9]+)\",\n",
    "    r\"(?i)ncbi\\.nlm\\.nih\\.gov/pubmed/\\?term=([0-9]+)\",\n",
    "    r\"(?i)[\\?&]term=([0-9]+)\",\n",
    "]\n",
    "\n",
    "NON_PUBMED_URL = re.compile(\n",
    "    r\"https?://(?!pubmed\\.ncbi\\.nlm\\.nih\\.gov|www\\.ncbi\\.nlm\\.nih\\.gov/pubmed)[^\\s|,;]+\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "BAN_PATTERNS = re.compile(\n",
    "    r\"\\b(?:CHEBI|PMCID|PMC|NCT|ISSN|ISBN|DB|CAS|EC)\\b[:#]?\\s*\\d+|\"\n",
    "    r\"\\bDOI[:\\s]*10\\.[^\\s|,;]+\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "BARE_PMID = re.compile(r\"\\b([0-9]{6,9})\\b\")  # conservative: 6–9 digits\n",
    "\n",
    "def extract_pmids_strict(text: str):\n",
    "    if not text:\n",
    "        return set()\n",
    "\n",
    "    # 1) explicit sources\n",
    "    explicit = set()\n",
    "    for pat in PMID_EXPLICIT:\n",
    "        explicit |= set(re.findall(pat, text))\n",
    "\n",
    "    # 2) clean text for bare-number detection\n",
    "    t = NON_PUBMED_URL.sub(\" \", text)       # drop numbers inside non-PubMed URLs\n",
    "    t = BAN_PATTERNS.sub(\" \", t)            # remove obvious non-PMID contexts\n",
    "\n",
    "    # if the cell has no PubMed cues and lots of words, be cautious with bare numbers\n",
    "    has_pubmed_cue = bool(re.search(r\"(?i)pmid|pubmed|ncbi\", text))\n",
    "\n",
    "    bare = set()\n",
    "    if has_pubmed_cue:\n",
    "        bare |= set(re.findall(BARE_PMID, t))\n",
    "    else:\n",
    "        # allow bare numbers only if the remaining text is mostly numbers/separators\n",
    "        stripped = re.sub(r\"[0-9,\\s|;/\\-]+\", \"\", t)\n",
    "        if stripped == \"\":\n",
    "            bare |= set(re.findall(BARE_PMID, t))\n",
    "\n",
    "    # normalize & dedupe\n",
    "    return {str(int(x)) for x in (explicit | bare)}  # strip leading zeros, keep as strings\n",
    "\n",
    "def verify_pmids(pmids, batch_size=200):\n",
    "    \"\"\"Verify via NCBI E-utilities (esummary).\"\"\"\n",
    "    valid, invalid = set(), set()\n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    headers = {\"User-Agent\": \"pmid-verifier/1.1\"}\n",
    "    pmids = list(pmids)\n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch = pmids[i:i+batch_size]\n",
    "        try:\n",
    "            r = requests.get(base, params={\"db\":\"pubmed\",\"id\":\",\".join(batch),\"retmode\":\"json\"},\n",
    "                             headers=headers, timeout=HTTP_TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            data = r.json().get(\"result\", {})\n",
    "            uids = set(data.get(\"uids\", []))\n",
    "            for p in batch:\n",
    "                (valid if p in uids and p in data else invalid).add(p)\n",
    "        except Exception:\n",
    "            invalid.update(batch)\n",
    "    return valid, invalid\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "def verify_vac_links(links, timeout=6):\n",
    "    \"\"\"\n",
    "    Returns sets of (valid, invalid).\n",
    "    A VAC link is 'valid' if the server responds with 2xx or 3xx.\n",
    "    \"\"\"\n",
    "    valid, invalid = set(), set()\n",
    "    headers = {\"User-Agent\": \"vac-link-checker/1.0\"}\n",
    "    for url in links:\n",
    "        try:\n",
    "            # try HEAD first\n",
    "            r = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            if r.status_code >= 400:  # fallback to GET\n",
    "                r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            if 200 <= r.status_code < 400:\n",
    "                valid.add(url)\n",
    "            else:\n",
    "                invalid.add(url)\n",
    "        except Exception:\n",
    "            invalid.add(url)\n",
    "    return valid, invalid\n",
    "\n",
    "\n",
    "\n",
    "def extract_vac_links(text: str):\n",
    "    if not text:\n",
    "        return set()\n",
    "    return set(re.findall(r\"https?://vac\\.niaid\\.nih\\.gov/view\\?id=\\d+\", text))\n",
    "\n",
    "# ---------- Run ----------\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "series = df[COL].astype(str).where(df[COL].notna(), \"\")\n",
    "\n",
    "empty_cells = 0\n",
    "cells_with_pmid = 0\n",
    "cells_with_vac = 0\n",
    "cells_other = 0\n",
    "\n",
    "all_pmids = set()\n",
    "all_vac = set()\n",
    "\n",
    "for cell in series:\n",
    "    s = cell.strip()\n",
    "    if not s:\n",
    "        empty_cells += 1\n",
    "        continue\n",
    "\n",
    "    pmids_here = extract_pmids_strict(s)\n",
    "    vac_here = extract_vac_links(s)\n",
    "\n",
    "    if pmids_here:\n",
    "        cells_with_pmid += 1\n",
    "        all_pmids |= pmids_here\n",
    "    if vac_here:\n",
    "        cells_with_vac += 1\n",
    "        all_vac |= vac_here\n",
    "    if not pmids_here and not vac_here:\n",
    "        cells_other += 1\n",
    "\n",
    "valid_pmids, invalid_pmids = verify_pmids(all_pmids)\n",
    "\n",
    "print(\"===== CELL CATEGORY STATS =====\")\n",
    "print(f\"Total cells:        {len(series)}\")\n",
    "print(f\"Empty:              {empty_cells}\")\n",
    "print(f\"Contains PMID:      {cells_with_pmid}\")\n",
    "print(f\"Contains VAC link:  {cells_with_vac}\")\n",
    "print(f\"Other (non-empty):  {cells_other}\")\n",
    "\n",
    "avg_pmids_per_pmid_cell = (len(all_pmids) / cells_with_pmid) if cells_with_pmid else 0\n",
    "print(\"\\n===== PMID STATS =====\")\n",
    "print(f\"Unique candidate PMIDs (strict): {len(all_pmids)}\")\n",
    "print(f\"Valid PMIDs:                    {len(valid_pmids)}\")\n",
    "print(f\"Invalid PMIDs:                  {len(invalid_pmids)}\")\n",
    "print(f\"Avg PMIDs per PMID-cell:        {avg_pmids_per_pmid_cell:.2f}\")\n",
    "\n",
    "# Run check\n",
    "valid_vac, invalid_vac = verify_vac_links(all_vac)\n",
    "\n",
    "print(\"\\n===== VAC LINK VALIDITY =====\")\n",
    "print(f\"Total VAC links: {len(all_vac)}\")\n",
    "print(f\"Valid VAC links: {len(valid_vac)}\")\n",
    "print(f\"Invalid VAC links: {len(invalid_vac)}\")\n",
    "avg_vacs_per_vac_cell = (len(all_vac) / cells_with_vac) if cells_with_vac else 0\n",
    "print(f\"Avg VACs per VAC-cell: {avg_vacs_per_vac_cell:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b515d14-a8a5-4933-be2d-d23376d6680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0795eb1-3b2a-4732-a1d7-10e43b78ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect the 19 \"Other (non-empty)\" cells ---\n",
    "other_cells = []\n",
    "\n",
    "for idx, cell in series.items():\n",
    "    s = cell.strip()\n",
    "    if s and not extract_pmids_strict(s) and not extract_vac_links(s):\n",
    "        other_cells.append((idx, s))\n",
    "\n",
    "print(f\"Found {len(other_cells)} 'Other' cells\")\n",
    "\n",
    "# Show first few for inspection\n",
    "for i, (row, text) in enumerate(other_cells[:22], start=1):\n",
    "    print(f\"\\n--- Other Cell #{i} (Row {row}) ---\")\n",
    "    print(text[:500])  # truncate long entries for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0d169-2fee-4b15-800f-ce97d2233b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "vac_path = \"Dataset/Vaxjo/VAC_September10_2025.xlsx\"\n",
    "\n",
    "# Regex + normalizer for VAC links\n",
    "VAC_ID_RE = re.compile(r\"https?://vac\\.niaid\\.nih\\.gov/view\\?id=(\\d+)\", re.I)\n",
    "def normalize_vac(url: str):\n",
    "    m = VAC_ID_RE.search(url or \"\")\n",
    "    return f\"https://vac.niaid.nih.gov/view?id={m.group(1)}\" if m else None\n",
    "\n",
    "# 1) Load workbook (first sheet), extract hyperlinks from column A\n",
    "wb = load_workbook(vac_path, data_only=True, read_only=False)\n",
    "ws = wb[wb.sheetnames[0]]\n",
    "\n",
    "# Some hyperlinks are attached to cells, some live in ws.hyperlinks (range-based)\n",
    "ref_to_target = {}\n",
    "for hl in getattr(ws, \"hyperlinks\", []):\n",
    "    if getattr(hl, \"target\", None) and getattr(hl, \"ref\", None):\n",
    "        ref_to_target[hl.ref] = hl.target\n",
    "\n",
    "found_vacs = set()\n",
    "for r in range(1, ws.max_row + 1):\n",
    "    url = None\n",
    "    cell = ws[f\"A{r}\"]\n",
    "    if cell.hyperlink and getattr(cell.hyperlink, \"target\", None):\n",
    "        url = cell.hyperlink.target\n",
    "    elif f\"A{r}\" in ref_to_target:\n",
    "        url = ref_to_target[f\"A{r}\"]\n",
    "    norm = normalize_vac(url)\n",
    "    if norm:\n",
    "        found_vacs.add(norm)\n",
    "\n",
    "print(f\"VAC links embedded in first column: {len(found_vacs)}\")\n",
    "\n",
    "# 2) Compare against your existing set `all_vac`\n",
    "#    (assumes you already have `all_vac` as the 64 unique VAC URLs)\n",
    "all_vac_norm = {normalize_vac(u) for u in all_vac if normalize_vac(u)}\n",
    "overlap = all_vac_norm & found_vacs\n",
    "missing = all_vac_norm - found_vacs\n",
    "extras  = found_vacs - all_vac_norm\n",
    "\n",
    "print(f\"Overlap with our {len(all_vac_norm)} VACs: {len(overlap)}\")\n",
    "print(f\"Missing (in our 64 but not in file): {len(missing)}\")\n",
    "print(f\"Extras (in file but not in our 64):  {len(extras)}\")\n",
    "\n",
    "# (Optional) peek\n",
    "print(\"Examples present:\", sorted(list(overlap))[:10])\n",
    "print(\"Examples missing:\", sorted(list(missing))[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60ad15-93ce-49c0-ace2-842dceb4e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "vac_file = \"Dataset/Vaxjo/VAC_September10_2025.xlsx\"\n",
    "\n",
    "# --- helpers ---\n",
    "VAC_ID_RE = re.compile(r\"https?://vac\\.niaid\\.nih\\.gov/view\\?id=(\\d+)\", re.I)\n",
    "\n",
    "def vac_id_from_url(url: str):\n",
    "    if not url:\n",
    "        return None\n",
    "    m = VAC_ID_RE.search(url)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "PMID_PATTERNS = [\n",
    "    r\"(?i)PMID[:\\s]*([0-9]+)\",\n",
    "    r\"(?i)Pubmed[:\\s]*([0-9]+)\",\n",
    "    r\"(?i)pubmed\\.ncbi\\.nlm\\.nih\\.gov/([0-9]+)\",\n",
    "    r\"(?i)[\\?&]term=([0-9]+)\",\n",
    "    r\"\\b([0-9]{5,9})\\b\",  # bare pmids (len 5–9)\n",
    "]\n",
    "def extract_pmids_cell(val) -> set:\n",
    "    if pd.isna(val):\n",
    "        return set()\n",
    "    text = str(val)\n",
    "    out = []\n",
    "    for pat in PMID_PATTERNS:\n",
    "        out += re.findall(pat, text)\n",
    "    # normalize (strip leading zeros), dedupe\n",
    "    return {str(int(x)) for x in out if x.isdigit()}\n",
    "\n",
    "# --- read the sheet with pandas ---\n",
    "df_vac = pd.read_excel(vac_file)  # first sheet by default\n",
    "\n",
    "# --- read embedded hyperlinks from column A (first column) with openpyxl ---\n",
    "wb = load_workbook(vac_file, data_only=True, read_only=False)\n",
    "ws = wb[wb.sheetnames[0]]\n",
    "\n",
    "# map range hyperlinks too (sometimes hyperlinks are stored in ws.hyperlinks)\n",
    "ref_to_target = {}\n",
    "for hl in getattr(ws, \"hyperlinks\", []):\n",
    "    if getattr(hl, \"target\", None) and getattr(hl, \"ref\", None):\n",
    "        ref_to_target[hl.ref] = hl.target\n",
    "\n",
    "vac_urls = []\n",
    "vac_ids  = []\n",
    "start_row = 2  # row 1 is header; pandas row 0 == Excel row 2\n",
    "for i in range(len(df_vac)):\n",
    "    addr = f\"A{start_row + i}\"\n",
    "    url = None\n",
    "    cell = ws[addr]\n",
    "    if cell.hyperlink and getattr(cell.hyperlink, \"target\", None):\n",
    "        url = cell.hyperlink.target\n",
    "    elif addr in ref_to_target:\n",
    "        url = ref_to_target[addr]\n",
    "    vac_urls.append(url)\n",
    "    vac_ids.append(vac_id_from_url(url))\n",
    "\n",
    "df_vac[\"VAC_URL\"] = vac_urls\n",
    "df_vac[\"VAC_ID\"]  = vac_ids\n",
    "\n",
    "# --- normalize your existing all_vac set to VAC_IDs ---\n",
    "# assumes you already have `all_vac` (the 64 VAC URLs) from earlier code\n",
    "all_vac_ids = {vac_id_from_url(u) for u in all_vac if vac_id_from_url(u)}\n",
    "\n",
    "# --- filter to rows that match our all_vac ---\n",
    "df_match = df_vac[df_vac[\"VAC_ID\"].isin(all_vac_ids)].copy()\n",
    "\n",
    "# --- keep the requested columns (rename if your headers differ) ---\n",
    "wanted_cols = [\"VAC_ID\", \"Preclinical PubMedID\", \"Clinical Trial PubMedID\"]\n",
    "vac_pmid_df = df_match[wanted_cols].copy()\n",
    "\n",
    "# --- build a unique PMID list from the two PMID columns ---\n",
    "unique_pmids = set()\n",
    "for col in [\"Preclinical PubMedID\", \"Clinical Trial PubMedID\"]:\n",
    "    if col in vac_pmid_df.columns:\n",
    "        for v in vac_pmid_df[col]:\n",
    "            unique_pmids |= extract_pmids_cell(v)\n",
    "\n",
    "unique_pmids = sorted(unique_pmids, key=int)\n",
    "\n",
    "print(f\"Rows matched to our VAC set: {len(vac_pmid_df)}\")\n",
    "print(f\"Unique PMIDs collected: {len(unique_pmids)}\")\n",
    "# peek\n",
    "print(len(unique_pmids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2d46f-0898-4e79-8942-a2f1d7cb2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_pmids (from your main parse) + unique_pmids (from VAC sheet)\n",
    "def _norm_ids(xs):\n",
    "    return {str(int(x)) for x in xs if str(x).strip().isdigit()}\n",
    "\n",
    "ultimate_pmids = sorted(_norm_ids(all_pmids) | _norm_ids(unique_pmids), key=int)\n",
    "\n",
    "print(f\"Ultimate unique PMIDs: {len(ultimate_pmids)}\")\n",
    "print(len(ultimate_pmids))  # peek\n",
    "\n",
    "# (optional) DataFrame / export\n",
    "# import pandas as pd\n",
    "# pd.DataFrame({\"PMID\": ultimate_pmids}).to_csv(\"ultimate_pmids.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c9f54-9f68-4990-9010-87858a872257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# ---------------------\n",
    "# 1. Fetch PubMed (title + abstract)\n",
    "# ---------------------\n",
    "def fetch_pubmed_info(pmids, batch_size=100, email=\"your.email@example.com\"):\n",
    "    \"\"\"\n",
    "    Fetch title + abstract for a list of PMIDs.\n",
    "    Returns dict: PMID -> {title, abstract}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    headers = {\"User-Agent\": \"pubmed-fetcher/1.0\"}\n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch = pmids[i:i+batch_size]\n",
    "        params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"id\": \",\".join(batch),\n",
    "            \"rettype\": \"abstract\",\n",
    "            \"retmode\": \"xml\",\n",
    "            \"email\": email\n",
    "        }\n",
    "        r = requests.get(base, params=params, headers=headers, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        for article in soup.find_all(\"pubmedarticle\"):\n",
    "            pmid = article.pmid.text\n",
    "            title = article.articletitle.text if article.articletitle else \"\"\n",
    "            abstract = \" \".join([ab.text for ab in article.find_all(\"abstracttext\")])\n",
    "            results[pmid] = {\"title\": title, \"abstract\": abstract}\n",
    "        time.sleep(0.4)  # throttle to be kind to NCBI\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# 2. Fetch VAC entry page\n",
    "# ---------------------\n",
    "def fetch_vac_info(vac_links):\n",
    "    \"\"\"\n",
    "    Fetch page <title> + main text for VAC entries.\n",
    "    Returns dict: VAC_URL -> {title, text}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    headers = {\"User-Agent\": \"vac-fetcher/1.0\"}\n",
    "    for url in vac_links:\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=15)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            title = soup.title.string if soup.title else \"\"\n",
    "            text = \" \".join(p.get_text(strip=True) for p in soup.find_all(\"p\"))\n",
    "            results[url] = {\"title\": title, \"text\": text[:5000]}  # limit text size\n",
    "        except Exception as e:\n",
    "            results[url] = {\"title\": \"\", \"text\": f\"ERROR: {e}\"}\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# 3. Fetch Other links\n",
    "# ---------------------\n",
    "def fetch_other_info(other_links):\n",
    "    \"\"\"\n",
    "    Generic scraper: title + paragraph text.\n",
    "    Returns dict: URL -> {title, text}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    headers = {\"User-Agent\": \"generic-scraper/1.0\"}\n",
    "    for url in other_links:\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=15)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            title = soup.title.string if soup.title else \"\"\n",
    "            text = \" \".join(p.get_text(strip=True) for p in soup.find_all(\"p\"))\n",
    "            results[url] = {\"title\": title, \"text\": text[:5000]}\n",
    "        except Exception as e:\n",
    "            results[url] = {\"title\": \"\", \"text\": f\"ERROR: {e}\"}\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# Example usage (assuming you already have sets all_pmids, all_vac, other_links)\n",
    "# ---------------------\n",
    "pmid_info = fetch_pubmed_info(sorted(ultimate_pmids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d90c6-1787-4369-8d10-41e329e04f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume pmid_info = fetch_pubmed_info(ultimate_pmids)\n",
    "\n",
    "empty_abstracts = [pid for pid, rec in pmid_info.items() if not rec.get(\"abstract\")]\n",
    "print(f\"Total PMIDs fetched: {len(pmid_info)}\")\n",
    "print(f\"Abstracts missing/empty: {len(empty_abstracts)}\")\n",
    "\n",
    "# peek at first few\n",
    "print(\"Examples:\", empty_abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a566c91-ce7b-4993-951f-0cf9bcfe320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "out_path = \"Dataset/Vaxjo/All PMID abstracts.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # compact JSON: no extra spaces or newlines\n",
    "    json.dump(pmid_info, f, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "\n",
    "print(f\"Wrote: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535973ec-2c08-4b44-a463-f06f67a077fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ultimate_pmids))\n",
    "print(isinstance(ultimate_pmids, list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee104e-9f2c-438a-95e3-31cecf08117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strings = all(isinstance(pmid, str) for pmid in ultimate_pmids)\n",
    "print(all_strings)  # True means all elements are strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9d954-f250-47f6-bae8-269e1995a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pmc-id-converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d591fc-1e3b-46ef-a244-8fdc07721de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "pmid_list = ultimate_pmids\n",
    "\n",
    "import requests\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "api_url = 'https://pmc.ncbi.nlm.nih.gov/tools/idconv/api/v1/articles/'\n",
    "\n",
    "# Add your email and tool name here\n",
    "tool_name = 'my_tool'\n",
    "\n",
    "email = \"your_email_here\"\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for batch in chunks(pmid_list, 200):\n",
    "    params = {\n",
    "        'ids': ','.join(batch),\n",
    "        'idtype': 'pmid',\n",
    "        'format': 'json',\n",
    "        'tool': tool_name,\n",
    "        'email': email\n",
    "    }\n",
    "    \n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    all_results.extend(data.get('records', []))\n",
    "\n",
    "# Now all_results contains PMCID mappings for all PMIDs\n",
    "for record in all_results:\n",
    "    print(f\"PMID: {record.get('pmid')}, PMCID: {record.get('pmcid')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b949e3-fbe4-42b6-b348-ea4174298a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with selected columns\n",
    "df = pd.DataFrame(all_results)[['pmid', 'pmcid']]\n",
    "\n",
    "# Rename columns if desired (optional)\n",
    "df.rename(columns={'pmid': 'PMID', 'pmcid': 'PMCID'}, inplace=True)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df.head(4))\n",
    "\n",
    "# Count how many PMCID are None or empty\n",
    "none_count = df['PMCID'].isna().sum()\n",
    "print(f\"Number of PMIDs with PMCID: {len(all_results)-none_count}\")\n",
    "print(f\"Number of PMIDs with no PMCID: {none_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147727a1-de33-4345-897b-21a9022f9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "os.makedirs('plaintext', exist_ok=True)\n",
    "os.makedirs('xml', exist_ok=True)\n",
    "\n",
    "base_url_xml = 'https://www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi'\n",
    "\n",
    "failed_xml = []\n",
    "failed_plaintext = []\n",
    "\n",
    "def download_plaintext(pmcid):\n",
    "    try:\n",
    "        url = f'https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/'\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            return r.text\n",
    "        else:\n",
    "            print(f\"Failed to download plaintext for {pmcid}: HTTP {r.status_code}\")\n",
    "            failed_plaintext.append(pmcid)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading plaintext for {pmcid}: {e}\")\n",
    "        failed_plaintext.append(pmcid)\n",
    "    return None\n",
    "\n",
    "def download_xml(pmcid):\n",
    "    article_id = pmcid.replace('PMC', '')\n",
    "    params = {\n",
    "        'verb': 'GetRecord',\n",
    "        'metadataPrefix': 'pmc',\n",
    "        'identifier': f'oai:pubmedcentral.nih.gov:{article_id}'\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(base_url_xml, params=params)\n",
    "        if r.status_code == 200:\n",
    "            return r.text\n",
    "        else:\n",
    "            print(f\"Failed to download xml for {pmcid}: HTTP {r.status_code}\")\n",
    "            failed_xml.append(pmcid)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading xml for {pmcid}: {e}\")\n",
    "        failed_xml.append(pmcid)\n",
    "    return None\n",
    "\n",
    "for pmcid in df['PMCID'].dropna().unique():\n",
    "    print(f\"Downloading {pmcid} ...\")\n",
    "\n",
    "    xml_content = download_xml(pmcid)\n",
    "    if xml_content:\n",
    "        with open(f'Dataset/Vaxjo/xml/{pmcid}.xml', 'w', encoding='utf-8') as f_xml:\n",
    "            f_xml.write(xml_content)\n",
    "\n",
    "    plaintext_content = download_plaintext(pmcid)\n",
    "    if plaintext_content:\n",
    "        with open(f'Dataset/Vaxjo/plaintext/{pmcid}.txt', 'w', encoding='utf-8') as f_txt:\n",
    "            f_txt.write(plaintext_content)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "#print(f\"\\nFailed XML downloads ({len(failed_xml)}): {failed_xml}\")\n",
    "#print(f\"Failed plaintext downloads ({len(failed_plaintext)}): {failed_plaintext}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a51c9-86f2-4bb3-aa2c-74b602f03988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save failed XML download IDs to a file\n",
    "with open('Dataset/Vaxjo/failed_xml_downloads.txt', 'w', encoding='utf-8') as f_xml_fail:\n",
    "    for pmcid in failed_xml:\n",
    "        f_xml_fail.write(pmcid + '\\n')\n",
    "\n",
    "# Save failed plaintext download IDs to a file\n",
    "with open('Dataset/Vaxjo/failed_plaintext_downloads.txt', 'w', encoding='utf-8') as f_txt_fail:\n",
    "    for pmcid in failed_plaintext:\n",
    "        f_txt_fail.write(pmcid + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10a9a8-9afc-4022-a49d-0d73916132ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
