{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0b962-c28b-4eee-ba9f-161ea4e52398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "LLM_FILE_PATH = 'outputs/Vaxjo_PMIDs_mechanism_summary_raw_outputs_llama3.2.jsonl'\n",
    "LLM_JSON_KEY = 'adjuvant'\n",
    "\n",
    "# Gold Standard Configuration\n",
    "GOLD_FILE_PATH = 'Dataset/Josh/vaccine_adjuvant_Master.xlsx'\n",
    "GOLD_SHEET_NAME = 'vaccine_adjuvant_VO_temmplate'\n",
    "GOLD_COL_LABEL = 'LABEL'\n",
    "GOLD_COL_ALT = 'alternative label'\n",
    "GOLD_ALT_SEPARATOR = '|'\n",
    "\n",
    "# --- Rule 1: Synonym Map ---\n",
    "SYNONYM_MAP = {\n",
    "    # Acronyms\n",
    "    'lipopolysaccharide': 'lps', 'oligodeoxynucleotide': 'odn', 'muramyl dipeptide': 'mdp',\n",
    "    'e. coli heat-labile toxin': 'lt', 'heat-labile toxin': 'lt', 'cholera toxin': 'ct',\n",
    "    'polyinosinic-polycytidylic acid': 'piclc', 'poly(i:c)': 'piclc', 'polyethyleneimine': 'pei',\n",
    "    'poly-ϵ-caprolactone': 'pcl', 'poly(dl-lactide-co-glycolide)': 'plga', 'chimpanzee adenovirus': 'chad',\n",
    "    # Common Names\n",
    "    'aluminum hydroxide': 'alum', 'aluminium hydroxide': 'alum', 'aluminum salts': 'alum', 'aluminium salts': 'alum',\n",
    "    # Domain Synonyms\n",
    "    'agonist': 'ligand', 'curdlan': 'beta-glucan', 'trehalose-6,6-dibehenate': 'tdb',\n",
    "    'wtlt': 'lt', 'mlt': 'lt', 'raspi': 'aspi'\n",
    "}\n",
    "\n",
    "# --- Rule 2: Prefixes/Suffixes ---\n",
    "PREFIXES_TO_STRIP = [\n",
    "    r'^m-', r'^r-', r'^h-', r'^p-', r'^wt-', r'^ov-',\n",
    "    r'^recombinant ', r'^murine ', r'^human ', r'^wild-type '\n",
    "]\n",
    "SUFFIXES_TO_STRIP = ['vaccine adjuvant', 'vaccine', 'adjuvants', 'adjuvant']\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def load_llm_names(file_path, key):\n",
    "    unique_names = set()\n",
    "    if not os.path.exists(file_path):\n",
    "        return unique_names\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    value = data.get(key)\n",
    "                    if not value:\n",
    "                        continue\n",
    "                    if isinstance(value, str):\n",
    "                        cleaned_value = value.strip()\n",
    "                        if cleaned_value:\n",
    "                            unique_names.add(cleaned_value)\n",
    "                    elif isinstance(value, list):\n",
    "                        for item in value:\n",
    "                            if isinstance(item, str):\n",
    "                                cleaned_item = item.strip()\n",
    "                                if cleaned_item:\n",
    "                                    unique_names.add(cleaned_item)\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading LLM file: {e}\")\n",
    "    return unique_names\n",
    "\n",
    "\n",
    "def load_gold_standard_entities(file_path, sheet_name, label_col, alt_label_col, separator):\n",
    "    gold_entities = []\n",
    "    if not os.path.exists(file_path):\n",
    "        return gold_entities\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        if label_col not in df.columns:\n",
    "            return gold_entities\n",
    "        print(f\"Loading gold standard entities from {len(df)} rows...\")\n",
    "        for index, row in df.iterrows():\n",
    "            raw_names = set()\n",
    "            primary_label = None\n",
    "            label = row.get(label_col)\n",
    "            if pd.notna(label):\n",
    "                cleaned = str(label).strip()\n",
    "                if cleaned:\n",
    "                    raw_names.add(cleaned)\n",
    "                    primary_label = cleaned\n",
    "            alt_labels = row.get(alt_label_col)\n",
    "            if pd.notna(alt_labels):\n",
    "                parts = str(alt_labels).split(separator)\n",
    "                for part in parts:\n",
    "                    cleaned = part.strip()\n",
    "                    if cleaned:\n",
    "                        raw_names.add(cleaned)\n",
    "            if raw_names:\n",
    "                strict_keys = set().union(*(get_normalized_keys_strict(name) for name in raw_names))\n",
    "                potential_keys = set().union(*(get_normalized_keys_potential(name) for name in raw_names))\n",
    "                if strict_keys or potential_keys:\n",
    "                    gold_entities.append({\n",
    "                        'raw_names': raw_names,\n",
    "                        'norm_keys_strict': strict_keys,\n",
    "                        'norm_keys_potential': potential_keys,\n",
    "                        'primary_label': primary_label if primary_label else next(iter(raw_names))\n",
    "                    })\n",
    "        print(f\"Successfully created {len(gold_entities)} gold standard entities.\")\n",
    "        return gold_entities\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file/sheet: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def _normalize_base(raw_name):\n",
    "    if not isinstance(raw_name, str) or not raw_name.strip():\n",
    "        return set()\n",
    "    base_names = {raw_name.lower()}\n",
    "\n",
    "    # Synonyms\n",
    "    temp_names = set()\n",
    "    sorted_synonyms = sorted(SYNONYM_MAP.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    for name in base_names:\n",
    "        for long, short in sorted_synonyms:\n",
    "            name = name.replace(long, short)\n",
    "        temp_names.add(name)\n",
    "    base_names = {n for n in temp_names if n.strip()}\n",
    "\n",
    "    # Expansions ((), [], /, +, and)\n",
    "    expanded = set()\n",
    "    for name in base_names:\n",
    "        paren_matches = re.findall(r'[\\(\\[](.*?)[\\)\\]]', name)\n",
    "        is_only_paren = re.fullmatch(r'[\\(\\[](.*?)[\\)\\]]', name) is not None\n",
    "        current_names_to_split = set()\n",
    "        if paren_matches:\n",
    "            for m in paren_matches:\n",
    "                expanded.add(m.strip())\n",
    "            if not is_only_paren:\n",
    "                outside = re.sub(r'[\\(\\[].*?[\\)\\]]', '', name).strip()\n",
    "                if outside:\n",
    "                    current_names_to_split.add(outside)\n",
    "        elif name:\n",
    "            current_names_to_split.add(name)\n",
    "\n",
    "        combo_expanded = set()\n",
    "        for n in current_names_to_split:\n",
    "            parts = re.split(r'\\s+and\\s+', n, flags=re.IGNORECASE)\n",
    "            for p in parts:\n",
    "                combo_expanded.update(re.split(r'[/\\+]', p))\n",
    "        expanded.update(p.strip() for p in combo_expanded if p.strip())\n",
    "    base_names = {n for n in expanded if n.strip()}\n",
    "\n",
    "    # Suffix stripping\n",
    "    suffixed_stripped_names = set()\n",
    "    for name in base_names:\n",
    "        clean_name = name\n",
    "        for suffix in SUFFIXES_TO_STRIP:\n",
    "            if clean_name.endswith(suffix):\n",
    "                clean_name = clean_name[:-len(suffix)]\n",
    "        clean_name = clean_name.strip()\n",
    "        if clean_name:\n",
    "            suffixed_stripped_names.add(clean_name)\n",
    "    base_names = suffixed_stripped_names\n",
    "\n",
    "    # Hyphen/space and alphanumeric forms\n",
    "    final_keys = set()\n",
    "    for name in base_names:\n",
    "        key1 = re.sub(r'[\\s\\-]+', ' ', name).strip()\n",
    "        key2 = re.sub(r'[^a-z0-9]', '', key1)\n",
    "        if key1:\n",
    "            final_keys.add(key1)\n",
    "        if key2 and key2 != key1:\n",
    "            final_keys.add(key2)\n",
    "    return final_keys\n",
    "\n",
    "\n",
    "def get_normalized_keys_strict(raw_name):\n",
    "    return _normalize_base(raw_name)\n",
    "\n",
    "\n",
    "def get_normalized_keys_potential(raw_name):\n",
    "    base_keys = _normalize_base(raw_name)\n",
    "    potential_keys = set()\n",
    "    for key in base_keys:\n",
    "        clean_key = key\n",
    "        for prefix in PREFIXES_TO_STRIP:\n",
    "            clean_key = re.sub(prefix, '', clean_key, flags=re.IGNORECASE)\n",
    "        clean_key = clean_key.strip()\n",
    "        if clean_key and clean_key != key:\n",
    "            potential_keys.add(clean_key)\n",
    "            key2 = re.sub(r'[^a-z0-9]', '', clean_key)\n",
    "            if key2 and key2 != clean_key:\n",
    "                potential_keys.add(key2)\n",
    "    return potential_keys\n",
    "\n",
    "\n",
    "def main():\n",
    "    raw_llm_names = load_llm_names(LLM_FILE_PATH, LLM_JSON_KEY)\n",
    "    gold_entities = load_gold_standard_entities(\n",
    "        GOLD_FILE_PATH, GOLD_SHEET_NAME, GOLD_COL_LABEL, GOLD_COL_ALT, GOLD_ALT_SEPARATOR\n",
    "    )\n",
    "    if not raw_llm_names or not gold_entities:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Starting Categorized Matching for {len(raw_llm_names)} LLM names ---\")\n",
    "\n",
    "    # Build maps\n",
    "    all_gold_strict_keys_map = {}\n",
    "    all_gold_potential_keys_map = {}\n",
    "    for i, entity in enumerate(gold_entities):\n",
    "        for key in entity['norm_keys_strict']:\n",
    "            all_gold_strict_keys_map.setdefault(key, set()).add(i)\n",
    "        for key in entity['norm_keys_potential']:\n",
    "            all_gold_potential_keys_map.setdefault(key, set()).add(i)\n",
    "\n",
    "    all_gold_strict_keys = set(all_gold_strict_keys_map)\n",
    "    all_gold_potential_keys = set(all_gold_potential_keys_map)\n",
    "\n",
    "    results = []\n",
    "    processed_llm_names = set()\n",
    "\n",
    "    for llm_name in raw_llm_names:\n",
    "        if llm_name in processed_llm_names:\n",
    "            continue\n",
    "\n",
    "        best_match_type = 'No Match'\n",
    "        best_match_entity_index = -1\n",
    "\n",
    "        strict_llm_keys = get_normalized_keys_strict(llm_name)\n",
    "        potential_llm_keys = get_normalized_keys_potential(llm_name)\n",
    "\n",
    "        # --- Exact match (strict) ---\n",
    "        found_exact = False\n",
    "        for llm_key in strict_llm_keys:\n",
    "            if llm_key in all_gold_strict_keys_map:\n",
    "                best_match_type = 'Exact'\n",
    "                best_match_entity_index = next(iter(all_gold_strict_keys_map[llm_key]))\n",
    "                found_exact = True\n",
    "                break\n",
    "        if found_exact:\n",
    "            gold_entity = gold_entities[best_match_entity_index]\n",
    "            primary = gold_entity['primary_label']\n",
    "            alts = '; '.join(sorted(gold_entity['raw_names'] - {primary}))\n",
    "            results.append({'LLM predicted': llm_name, 'Gold Label': primary,\n",
    "                            'gold Alternative': alts or None, 'Match': best_match_type})\n",
    "            processed_llm_names.add(llm_name)\n",
    "            continue\n",
    "\n",
    "        # --- Partial match (substring containment) ---\n",
    "        found_partial = False\n",
    "        for llm_key in strict_llm_keys:\n",
    "            for gold_key in all_gold_strict_keys:\n",
    "                if not llm_key or not gold_key:\n",
    "                    continue\n",
    "                if llm_key in gold_key or gold_key in llm_key:\n",
    "                    best_match_type = 'Partial'\n",
    "                    best_match_entity_index = next(iter(all_gold_strict_keys_map[gold_key]))\n",
    "                    found_partial = True\n",
    "                    break\n",
    "            if found_partial:\n",
    "                break\n",
    "\n",
    "        if found_partial:\n",
    "            gold_entity = gold_entities[best_match_entity_index]\n",
    "            primary = gold_entity['primary_label']\n",
    "            alts = '; '.join(sorted(gold_entity['raw_names'] - {primary}))\n",
    "            results.append({'LLM predicted': llm_name, 'Gold Label': primary,\n",
    "                            'gold Alternative': alts or None, 'Match': best_match_type})\n",
    "            processed_llm_names.add(llm_name)\n",
    "            continue\n",
    "\n",
    "        # --- Potential match (prefix-stripped keys) ---\n",
    "        effective_potential_llm_keys = potential_llm_keys - strict_llm_keys\n",
    "        found_potential = False\n",
    "        for llm_key in effective_potential_llm_keys:\n",
    "            if llm_key in all_gold_potential_keys_map:\n",
    "                best_match_type = 'Potential'\n",
    "                best_match_entity_index = next(iter(all_gold_potential_keys_map[llm_key]))\n",
    "                found_potential = True\n",
    "                break\n",
    "\n",
    "        if found_potential:\n",
    "            gold_entity = gold_entities[best_match_entity_index]\n",
    "            primary = gold_entity['primary_label']\n",
    "            alts = '; '.join(sorted(gold_entity['raw_names'] - {primary}))\n",
    "            results.append({'LLM predicted': llm_name, 'Gold Label': primary,\n",
    "                            'gold Alternative': alts or None, 'Match': best_match_type})\n",
    "            processed_llm_names.add(llm_name)\n",
    "            continue\n",
    "\n",
    "        # --- No Match ---\n",
    "        results.append({'LLM predicted': llm_name, 'Gold Label': None,\n",
    "                        'gold Alternative': None, 'Match': 'No Match'})\n",
    "        processed_llm_names.add(llm_name)\n",
    "\n",
    "    # --- Results ---\n",
    "    final_df = pd.DataFrame(results)\n",
    "    order = {'Exact': 1, 'Partial': 2, 'Potential': 3, 'No Match': 4, 'Normalization Failed': 5}\n",
    "    final_df['Sort_Order'] = final_df['Match'].map(order)\n",
    "    final_df = final_df.sort_values(by=['Sort_Order', 'LLM predicted']).reset_index(drop=True)\n",
    "    final_df = final_df.drop(columns=['Sort_Order'])\n",
    "\n",
    "    print(f\"\\n--- Final Categorized Matching DataFrame ({len(final_df)} LLM names analyzed) ---\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "        print(final_df)\n",
    "\n",
    "    final_df.to_csv(\"outputs/llm_match_analysis_categorized_final.csv\", index=False)\n",
    "    # Save results to Excel instead of CSV\n",
    "    output_excel_path = \"outputs/llm_match_analysis_categorized_final.xlsx\"\n",
    "    final_df.to_excel(output_excel_path, index=False, sheet_name=\"Match Results\")\n",
    "    \n",
    "    print(f\"\\nResults saved to Excel: {output_excel_path}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Match Category Summary ---\")\n",
    "    print(final_df['Match'].value_counts())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d1278-ea21-4b0e-bbe7-0460e6d97ee0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Josh went through this list and found that \"I reviewed the terms in the \"No Match\" category and found 27/44 should be added as new adjuvants that are not already in Vaxjo! The others were mostly duplicates. This should complete the final list of vaccine adjuvants. I added two columns, one is \"new potential adjuvant\" and in that I describe if it should be added to vaxjo or not and why. The second column is \"pmid\" and has the pmid source to be listed in VO if it is a new adjuvant that should be added. \"\n",
    "I have uploaded this file as \"outputs/llm_match_analysis_categorized_final.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ed9b9-7453-475c-9365-f98afaeb5938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0b160-814e-4ce3-8f78-8366a6d26dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019f3f9-d103-4677-98c1-35410d403a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- File paths ---\n",
    "EXCEL_PATH = \"outputs/llm_match_analysis_categorized_final_Josh.xlsx\"  # Josh’s reviewed file\n",
    "TXT_PATH = \"outputs/Vaxjo_PMIDs_mechanism_summary_raw_outputs_llama3.2.txt\"\n",
    "OUTPUT_PATH = \"outputs/llm_match_analysis_categorized_final_Josh_with_summary_mechanisms.xlsx\"\n",
    "\n",
    "# --- Helper: Robust JSON extraction (brace-balancing) ---\n",
    "def extract_json_objects(text):\n",
    "    objs = []\n",
    "    depth = 0\n",
    "    start = None\n",
    "    in_string = False\n",
    "    esc = False\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        if in_string:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == '\\\\':\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_string = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_string = True\n",
    "                continue\n",
    "            if ch == '{':\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    candidate = text[start:i+1]\n",
    "                    try:\n",
    "                        objs.append(json.loads(candidate))\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                    start = None\n",
    "    return objs\n",
    "\n",
    "\n",
    "# --- Load and parse the TXT file ---\n",
    "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "records = extract_json_objects(txt)\n",
    "\n",
    "# --- Build lookup maps ---\n",
    "def norm(s):\n",
    "    return s.strip().lower() if isinstance(s, str) else None\n",
    "\n",
    "summary_map = {}\n",
    "mech_map = {}\n",
    "\n",
    "for rec in records:\n",
    "    adj = rec.get(\"adjuvant\")\n",
    "    if not adj:\n",
    "        continue\n",
    "    key = norm(adj)\n",
    "    summary_map[key] = rec.get(\"summary\")\n",
    "    mech_map[key] = json.dumps(rec.get(\"mechanism_subtypes\", []), ensure_ascii=False)\n",
    "\n",
    "# --- Load Josh’s Excel and enrich it ---\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "df[\"__key__\"] = df[\"LLM predicted\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "df[\"LLM summary\"] = df[\"__key__\"].map(summary_map)\n",
    "df[\"LLM mechanism_subtypes\"] = df[\"__key__\"].map(mech_map)\n",
    "\n",
    "# --- Sorting: Bring “yes” or relevant entries to top ---\n",
    "df_sorted = df.sort_values(by=\"New Potential Adjuvant?\", ascending=False)\n",
    "\n",
    "# --- Diagnostics ---\n",
    "total = len(df)\n",
    "matched = df[\"LLM summary\"].notna().sum()\n",
    "print(f\"Parsed {len(records)} JSON blocks from TXT.\")\n",
    "print(f\"Matched {matched}/{total} adjuvants from Josh’s Excel.\")\n",
    "if matched < total:\n",
    "    print(\"Unmatched examples:\")\n",
    "    print(df.loc[df['LLM summary'].isna(), 'LLM predicted'].head(10).tolist())\n",
    "\n",
    "# --- Save final output ---\n",
    "df_sorted.drop(columns=[\"__key__\"], inplace=True)\n",
    "df_sorted.to_excel(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"\\n✅ Final enriched file saved to:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc88df7-cd64-406c-86a1-2070c77e97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- File paths ---\n",
    "EXCEL_PATH = \"outputs/llm_match_analysis_categorized_final_Josh.xlsx\"\n",
    "TXT_PATH = \"outputs/Vaxjo_PMIDs_mechanism_summary_raw_outputs_llama3.2.txt\"\n",
    "OUTPUT_PATH = \"outputs/llm_match_analysis_categorized_final_Josh_with_summary_mechanisms_raw_multiline.xlsx\"\n",
    "\n",
    "# --- Helper: Robust JSON extraction ---\n",
    "def extract_json_objects(text):\n",
    "    objs = []\n",
    "    depth = 0\n",
    "    start = None\n",
    "    in_string = False\n",
    "    esc = False\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        if in_string:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == '\\\\':\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_string = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_string = True\n",
    "                continue\n",
    "            if ch == '{':\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    candidate = text[start:i+1]\n",
    "                    try:\n",
    "                        objs.append(json.loads(candidate))\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                    start = None\n",
    "    return objs\n",
    "\n",
    "\n",
    "# --- Load TXT file ---\n",
    "with open(TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "records = extract_json_objects(txt)\n",
    "\n",
    "# --- Build lookup maps ---\n",
    "def norm(s):\n",
    "    return s.strip().lower() if isinstance(s, str) else None\n",
    "\n",
    "summary_map = {}\n",
    "mech_map = {}\n",
    "\n",
    "for rec in records:\n",
    "    adj = rec.get(\"adjuvant\")\n",
    "    if not adj:\n",
    "        continue\n",
    "    key = norm(adj)\n",
    "    summary_map[key] = rec.get(\"summary\")\n",
    "    mechanisms = rec.get(\"mechanism_subtypes\", [])\n",
    "    if mechanisms:\n",
    "        mech_map[key] = \"\\n\".join(json.dumps(m, ensure_ascii=False) for m in mechanisms)\n",
    "    else:\n",
    "        mech_map[key] = None\n",
    "\n",
    "# --- Load Josh’s Excel ---\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "df[\"__key__\"] = df[\"LLM predicted\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "df[\"LLM summary\"] = df[\"__key__\"].map(summary_map)\n",
    "df[\"LLM mechanism_subtypes\"] = df[\"__key__\"].map(mech_map)\n",
    "\n",
    "# --- Sort by “New Potential Adjuvant?” ---\n",
    "df_sorted = df.sort_values(by=\"New Potential Adjuvant?\", ascending=False)\n",
    "\n",
    "# --- Save ---\n",
    "df_sorted.drop(columns=[\"__key__\"], inplace=True)\n",
    "df_sorted.to_excel(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Saved Excel with raw JSON subtypes separated by newlines:\\n{OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b203995-0a24-408d-946b-4ddec658b9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tavr-monai",
   "language": "python",
   "name": "tavr-monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
